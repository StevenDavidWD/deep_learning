### Note-2
---
#### Page70～Page93
#### 2017年8月8日
---
#### 不确定性的来源
机器学习任务中通常包含了很多的不确定性，而这些不确定性的来源通常有以下几种：
+ 被建模系统内部的随机性
+ 不完全观测(信息不对称)
+ 不完全建模(不得已舍弃了部分的信息)

#### 频率派概率与贝叶斯概率
概率的含义通常有多种解释，主要分为两类：
+ 概率与事件发生的频率相关，被称为频率派概率
+ 概率涉及确定性水平，被称为贝叶斯概率

#### 条件概率的链式法则
任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相乘的形式：

$$P(\mathrm{x^{(1)}},...\mathrm{x^{(n)}})=P(\mathrm{x^{(1)}})\prod_{i=2}^{n}P(\mathrm{x^{(i)}}|\mathrm{x^{(1)}},...\mathrm{x^{(i-1)}})$$

#### 协方差
协方差在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度：
$$Cov(f(x), g(y)) = \mathbb{E}[(f(x)-\mathbb{E}[f(x)])(g(y)-\mathbb{E}[g(y)])]$$
+ 如果协方差的绝对值很大，则意味着变量变化很大**且他们同时距离各自均值很远**
+ 如果协方差是正的，**则两个变量都同时倾向于取较大的值**
+ 如果协方差是负的，**则其中一个变量倾向于取较大值的同时，另一个变量倾向于取较小值**

#### 相关系数
$$\eta=\cfrac{Cov(f(x),g(y))}{\sigma_{x}\cdot\sigma_{y}}$$
归一化了协方差中关于变量尺度的部分，只描述相关性(线性相关性)
#### 协方差与相关性和独立性的关系
随机变量的相关性指的是线性相关性，而独立性则要求更严。判断两变量的是否独立还要检验其是否存在非线性关系。
+ 协方差不为零，则两变量一定是相关的；协方差为零，则两变量一定不相关；
+ 两变量相互独立，则协方差为零；两变量不相互独立，协方差不一定不为零；

#### $Multinoulli$分布
$Multinoulli$分布或者称为范畴分布是伯努利($Bernoulli$)分布在高维空间上的推广，其指的是具有k个不同状态的单个离散型随机变量上的分布，k为有限值。

其概率质量函数可写为如下形式：
$$Mu(\mathrm{X}|1,\theta)=\prod^{k}_{i=1}\theta_i^{\coprod(x_i=1)}$$
其中：
$$\coprod(p)=\begin{cases}1&\mbox{if event p is true}\\0&\mbox{if event p is false}\end{cases}$$

$$\mathrm{X}=\begin{bmatrix}
x_1\\
\vdots\\
x_k\\
  \end{bmatrix}
$$
$$\mathrm{\theta}=\begin{bmatrix}
\theta_1\\
\vdots\\
\theta_k\\
  \end{bmatrix}
$$
且
$$\mathrm{X}\in\{0, 1\}^k，\sum_{j=1}^kx_j=1$$
$$\theta\in\mathbb{R}^k，\sum_{j=1}^k\theta_j=1$$

此式表明，$\mathrm{X}$ 描述了一组随机变量可能的取值，而在某一时刻时，此随机变量只能在某一个维度上的量为真。其对应的概率密度向量为 $\theta$ ，表示了特定维度为真的概率。

#### 多维正态分布
$$\mathcal{N}(x;\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$

其中，$\mu$ 为各随机变量均值所组成的向量，$\Sigma$ 为各随机变量的协方差矩阵。

#### 概率常用函数
Logistic sigmoid函数:
$$\sigma(x)=\frac{1}{1+exp(-x)}$$
常用来产生伯努利分布中的参数 $\phi$

Softplus函数：
$$\zeta(x)=log(1+exp(x))$$
常用来生成正态分布的参数 $\beta$ 和 $\sigma$
