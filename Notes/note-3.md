### Note-3
---
#### Page94～Page
#### 2017年8月10日
---
#### 自信息
我们定义一个事件: $\mathrm{x}=xs$的自信息为：
$$I(x)=-\log P(x)$$
若底数为$e$，则信息量单位为**奈特(nats)**

若底数为$2$，则信息量单位为**比特(bits)**

#### 香农熵
香农熵对整个概率分布中的不确定性总量进行了量化：
$$H(x)=\mathbb{E}_{x\sim P}[I(x)]=-\mathbb{E}_{x\sim P}[\log P(x)]$$
一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。

当 $\mathrm{x}$ 为连续的，香农熵被称为微分熵。

#### KL散度
对于同一个随机变量 $\mathrm{x}$ ，有单独的两个概率分布 $P(x)$ 和 $Q(x)$ ，我们可以使用KL散度来衡量这两个分布的差异
$$D_{KL}(P\|Q)=\mathbb{E}_{\mathrm{x}\sim P}[\log \frac{P(x)}{Q(x)}]=\mathbb{E}_{\mathrm{x}\sim P}[\log P(x)-\log Q(x)]$$

值得注意的是，$D_{KL}(P\|Q)\ne D_{KL}(Q\|P)$，具体选用何种方向需要考虑应用场景。

另一个与KL散度密切联系的量是交叉熵
$$H(P,Q)=H(P)+D_{KL}(P\|Q)=-\mathbb{E}_{\mathrm{x}\sim P}\log Q(x)$$

#### 结构化概率模型
在机器学习任务中，会经常要研究涉及非常多变量上的概率分布，难以使用单个函数来描述整个联合概率分布。

因此，一个正常的想法是利用概率的乘法公式，将概率表示成连乘的形式。
$$p(a,b,c)=p(a)p(b\mid a)p(c\mid a,b)$$

而这样一种点稀疏，关系密切的情况可以用离散数学中的**图**来很好的描述

#### 条件数与病态条件
条件数用来表征函数相对于输入的微小变化而变化的快慢程度。

当函数的输出因为输入的轻微扰动而发生巨大变化时，我们称这个函数是病态条件的。

#### 各种函数名
在机器学习中，我们通常把要最小化或者最大化的函数称为**目标函数**或者**准则函数**；

当学习任务是将他们最小化时，这些函数又被称为**代价函数**，**损失函数**或者**误差函数**；

#### 鞍点
同时存在更高和更低相邻点且 $f'(x)=0$ 的点

#### 线搜索策略
使用梯度下降法确定步长 $\epsilon$ 时，可以同时设置多个 $\epsilon$ 值，同时计算 $f(x-\epsilon\nabla_xf(x))$，最后选择能产生最小目标函数的 $\epsilon$
